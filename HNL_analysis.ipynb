{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5966fc77-70ae-46ee-be8e-6cc3c3cf4d63",
   "metadata": {},
   "source": [
    "# HNL CMS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812f18e-7d06-4983-ac20-b4128555b354",
   "metadata": {},
   "source": [
    "Code from Leonardo Lunerti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e0a75-80b8-43a8-bc4f-e2f499c9e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed = False ## Default True - Enable DASK parallelisation\n",
    "\n",
    "if distributed:\n",
    "    sched_port = 22690 ## Change this from the DASK cluster information panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b38a46-7699-43dd-a9a7-38e3a067c8b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask cluster configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a53b05-78d6-449f-88f5-b18ab8b3ba67",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTE**: The cell below must be changed every time the Dask cluster is recreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aeea3-0f3f-4a92-b3ed-224d5336263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "if distributed:  \n",
    "    client = Client(\"localhost:\" + str(sched_port))\n",
    "    client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388dbb-997e-4d0a-a7be-c6cf9aaa6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.restart() #Execute this only to restart the workers (to relaunch the notebook, for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5dab5-15b0-4ca4-b2e9-6282d0d29a12",
   "metadata": {},
   "source": [
    "Upload the X509 proxy in the Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8a77a-756f-4ed6-a429-f7056b1d25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.diagnostics.plugin import UploadFile\n",
    "if distributed:\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de92c3-886c-44be-8ddf-536b1be51387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a391700-c289-4ca5-ab41-48fb85b6387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.run(set_proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839e2e6-80e8-416c-bd21-33d4b482e0f3",
   "metadata": {},
   "source": [
    "Clear workers from any residual ROOT files (previous runs) and auxiliary files (SFs and configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a971e05-dfd1-409c-aa85-b2204ce292f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_nodes(dask_worker, with_aux_files=False):\n",
    "    import os\n",
    "    os.popen('rm ./*.root')\n",
    "    os.popen('rm ./*.csv')\n",
    "    if with_aux_files:\n",
    "        os.popen('rm -r ./aux_files')\n",
    "        os.popen('rm -r ./python')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508ae76-5937-4350-aaea-a3e79e790f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.run(clear_nodes, with_aux_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ef64c-9349-47a5-9d83-9bf7a795cf16",
   "metadata": {},
   "source": [
    "Define the operations for moving the output files to remote storage (via Davix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad904b4c-0216-4f48-b3e7-9e38d301f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_tier(dask_worker=None, remote_folder_name=\"\"):\n",
    "    import os\n",
    "    os.popen('for filename in ./*.root; do davix-put \"$filename\" {}/\"$filename\" -E $X509_USER_PROXY --capath $X509_CERT_DIR; done'.format(remote_folder_name))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085090d-c0b7-4144-910e-824db10c42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_tier_monitoring(dask_worker=None, remote_folder_name=\"\"):\n",
    "    import os\n",
    "    os.popen('for filename in ./distrdf_*.csv; do davix-put \"$filename\" {}/monitoring/\"$filename\" -E $X509_USER_PROXY --capath $X509_CERT_DIR; done'.format(remote_folder_name))\n",
    "    #os.popen('for filename in ./*.csv; do davix-put \"$filename\" {}/\"$filename\" -E $X509_USER_PROXY --capath $X509_CERT_DIR; done'.format(remote_folder_name))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057086b-8a14-49c6-bedf-9f1ca22b2077",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import modules and local X509 proxy configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02e274-2650-48ac-87f9-230887e5b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from python import hnl_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5ed4c-1531-4760-a622-6264d3262a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading PROXY locally\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c79fd-39d3-49a1-b0c9-e52ee0d81514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opts for Lazy snapshot\n",
    "#opts = ROOT.RDF.RSnapshotOptions()\n",
    "#opts.fLazy = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eace4cf-92e0-4c1b-b287-8f0d1c4e7e3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HNL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448af553-5dfb-4cbc-bc4c-eea6223a9fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set the analysis initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcbf61-1adc-415b-b6f5-6940411d2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_start = time.time()\n",
    "# Mandatory root files\n",
    "configFileName = \"/opt/workspace/persistent-storage/AF_HNL_analysis/cfg/DsToHnlMu_HnlToMuPi_prompt_UL_crab_cfg_dask.json\"\n",
    "dataset_to_process = \"DsToNMu_NToMuPi_mN1p5_ctau1000p0mm_incl\" #\"ParkingBPH1_D\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451009e-b3ae-4088-8f83-034e5a198a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code options\n",
    "saveOutputTree        = True  # Default False - Save an output tree (after all the cuts)\n",
    "savePreselectedTree   = True  # Default False - Save an output tree (before the cuts and after best candidate selection)\n",
    "doHistograms          = False #True Default False - Do not save histogram as output\n",
    "addSPlotWeight        = False # Default False - Add splot weight for data \n",
    "skipPUrw              = False # Default False - Do not apply PU reweighting\n",
    "skipTrigSF            = False # Default False - Do not apply trigger scale factors\n",
    "skipMuIDsf            = False # Default False - Do not apply muon id scale factors\n",
    "skipMuRecosf          = False # Default False - Do not apply muon reco scale factors\n",
    "nThreads              = 1     # Default 1     - Number of threads\n",
    "tag                   = \"\"    # Default \"\"    - Tag output files\n",
    "ctauReweighting       = False # Default False - Include ctau reweighting\n",
    "applyMuDsPtCorr       = False # Default False - Apply reweighting to mu from Ds to correct data/MC pt discrepancies\n",
    "applyMuHnlPtCorr      = False # Default False - Apply reweighting to mu from Hnl to correct data/MC pt discrepancies\n",
    "applyMuDsIPSCorr      = False # Default False - Apply reweighting to mu from Ds to correct data/MC IPS discrepancies\n",
    "applyMuHnlIPSCorr     = False # Default False - Apply reweighting to mu from Hnl to correct data/MC IPS discrepancies\n",
    "bestCandChecks        = False # Default False - Make checks for best candidate selection\n",
    "varyMuIDSf            = 0.0   # Default 0.0 - Muon ID sf w/ variations: sf = sf+variation*error\n",
    "varyMuRecoSf          = 0.0   # Default 0.0 - Muon reco sf w/ variations: sf = sf+variation*error\n",
    "keep                  = []    # Default []    - Select which branches to keep in the final output tree\n",
    "saveRemotely          = False # Default False - Enables remote save with davix - MANDATORY IF DISTRIBUTED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a03a18-8f16-464a-a946-a8e9cd8c2c55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Open the configuration files and get the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9deb7-387f-452c-80c4-db0b7621ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open analyzer configuration file\n",
    "with open(configFileName, \"r\") as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "#get ntuples configuration\n",
    "with open(config[\"ntuples_cfg_file_full_path\"], \"r\") as f:\n",
    "    ntuples = json.loads(f.read())\n",
    "\n",
    "#get histogram configuration\n",
    "#with open(config[\"histogram_cfg_file_full_path\"], \"r\") as f:\n",
    "#    histos = json.loads(f.read())\n",
    "\n",
    "#get selection and categorization cuts\n",
    "with open(config[\"selection_cfg_file_full_path\"], \"r\") as f:\n",
    "    selection = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438ac8e-75a9-4d58-bc69-9341b3ffe968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get input files\n",
    "\n",
    "input_file_list = \"file_name_list\"\n",
    "tree_name = \"wztree\"\n",
    "\n",
    "#if skipSlimCuts:\n",
    "#    input_file_list = \"slimmed_file_name_list\"\n",
    "#    tree_name = \"slimmed_tree\"\n",
    "#\n",
    "#if addSPlotWeight and skipSelCuts and skipSlimCuts:\n",
    "#    input_file_list = \"final_file_name_list\"\n",
    "#    tree_name = \"final_tree\"\n",
    "\n",
    "#get input files\n",
    "inputFileName_list = ntuples[dataset_to_process][input_file_list]\n",
    "dataset_category = ntuples[dataset_to_process][\"dataset_category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5ad48-79d8-4f92-858a-50adf8f49f56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push and set auxiliary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d868f9-bdc8-4799-b877-200c52a39b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.register_worker_plugin(UploadFile(config[\"pu_weight_input_file_signal\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"pu_weight_input_file_bkg\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"mu_id_sf_input_file\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"mu_reco_sf_input_file\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"trigger_sf_input_file\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"trigger_eff_data_input_file\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"trigger_eff_mc_input_file\"]))\n",
    "    client.register_worker_plugin(UploadFile(config[\"hnl_tools\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb034068-7770-4f42-b143-bba0a579267d",
   "metadata": {},
   "source": [
    "Move the auxiliary files in the HOME directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a2176-2ee1-4388-8562-e4b47039541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test(dask_worker):\n",
    "#    import os\n",
    "#    return os.listdir(\"./\")\n",
    "#\n",
    "#client.run(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571779e-8742-4af5-ad14-fd48da6c3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_auxiliary_files(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    os.mkdir(working_dir + '/../../../aux_files/')\n",
    "    os.mkdir(working_dir + '/../../../python/')\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"pu_weight_input_file_signal\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"pu_weight_input_file_signal\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"pu_weight_input_file_bkg\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"pu_weight_input_file_bkg\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"mu_id_sf_input_file\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"mu_id_sf_input_file\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"mu_reco_sf_input_file\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"mu_reco_sf_input_file\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"trigger_sf_input_file\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"trigger_sf_input_file\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"trigger_eff_data_input_file\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"trigger_eff_data_input_file\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"trigger_eff_mc_input_file\"].split(\"/\")[-1] , working_dir + '/../../../aux_files/' + config[\"trigger_eff_mc_input_file\"].split(\"/\")[-1])\n",
    "    shutil.copyfile(working_dir + \"/\" + config[\"hnl_tools\"].split(\"/\")[-1] , working_dir + '/../../../python/' + config[\"hnl_tools\"].split(\"/\")[-1])\n",
    "    return os.listdir(\"/srv/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692918a-e795-41c2-9ec0-3e67eb4ec4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    client.run(set_auxiliary_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c71151-133f-4792-a929-1d38e12b9881",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Declare custom functions in RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb81ac-865c-4948-9345-6ce3b2e184c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let private function to be avaliable in RDataFrame evn\n",
    "text_file = open(config[\"user_defined_function_path\"], \"r\")\n",
    "data = text_file.read()\n",
    "\n",
    "def my_initialization_function():\n",
    "    ROOT.gInterpreter.AddIncludePath(\"/cvmfs/cms.dodas.infn.it/boost_1_77_0\")\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    #get pu weights histogram\n",
    "    if dataset_category != \"data\" and not skipPUrw:\n",
    "        if dataset_category==\"signal\":\n",
    "            ROOT.gInterpreter.Declare(\"\"\"\n",
    "            auto pu_weights_file = TFile::Open(\"{}\");\n",
    "            auto h_pu_weights = pu_weights_file->Get<TH1D>(\"{}\");\n",
    "            \"\"\".format(config[\"pu_weight_node_input_file_signal\"],config[\"pu_weight_histo_name\"])\n",
    "            )\n",
    "        else:\n",
    "            ROOT.gInterpreter.Declare(\"\"\"\n",
    "            auto pu_weights_file = TFile::Open(\"{}\");\n",
    "            auto h_pu_weights = pu_weights_file->Get<TH1D>(\"{}\");\n",
    "            \"\"\".format(config[\"pu_weight_node_input_file_bkg\"],config[\"pu_weight_histo_name\"])\n",
    "            )\n",
    "    \n",
    "    #get trigger scale factors histogram\n",
    "    if dataset_category != \"data\" and not skipTrigSF:\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        auto trigger_eff_data_file = TFile::Open(\"{edata_file}\");\n",
    "        auto trigger_eff_mc_file   = TFile::Open(\"{emc_file}\");\n",
    "        auto h_trigger_eff_data = trigger_eff_data_file->Get<TH2D>(\"{edatahisto_name}\");\n",
    "        auto h_trigger_eff_mc   = trigger_eff_mc_file->Get<TH2D>(\"{emchisto_name}\");\n",
    "        \"\"\".format(edata_file=config[\"trigger_eff_data_node_input_file\"],\n",
    "                   emc_file=config[\"trigger_eff_mc_node_input_file\"],\n",
    "                   edatahisto_name=config[\"trigger_eff_data_histo_name\"],\n",
    "                   emchisto_name=config[\"trigger_eff_mc_histo_name\"])\n",
    "        )\n",
    "\n",
    "    #get pu weights histogram\n",
    "    if dataset_category != \"data\" and (not skipMuIDsf or not skipMuRecosf):\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        #include <boost/property_tree/ptree.hpp>\n",
    "        #include <boost/property_tree/json_parser.hpp>\n",
    "        \"\"\"\n",
    "        )\n",
    "        if not skipMuIDsf:\n",
    "            ROOT.gInterpreter.Declare(\"boost::property_tree::ptree mu_id_sf_cfg;\")\n",
    "            ROOT.gInterpreter.ProcessLine(\"\"\"\n",
    "            boost::property_tree::read_json(\"{infile}\",mu_id_sf_cfg);\n",
    "            \"\"\".format(infile=config[\"mu_id_sf_node_input_file\"])\n",
    "            )\n",
    "        if not skipMuRecosf:\n",
    "            ROOT.gInterpreter.Declare(\"boost::property_tree::ptree mu_reco_sf_cfg;\")\n",
    "            ROOT.gInterpreter.ProcessLine(\"\"\"\n",
    "            boost::property_tree::read_json(\"{infile}\",mu_reco_sf_cfg);\n",
    "            \"\"\".format(infile=config[\"mu_reco_sf_node_input_file\"])\n",
    "            )\n",
    "        \n",
    "    #get mu_Ds pt shape scale factors histogram\n",
    "    if dataset_category != \"data\" and applyMuDsPtCorr:\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        auto ds_pt_shape_sf_file = TFile::Open(\"{}\");\n",
    "        auto h_ds_pt_shape_sf = ds_pt_shape_sf_file->Get<TH1D>(\"{}\");\n",
    "        \"\"\".format(config[\"ds_pt_shape_sf_input_file\"],config[\"ds_pt_shape_sf_histo_name\"])\n",
    "        )\n",
    "    #get mu_Hnl pt shape scale factors histogram\n",
    "    if dataset_category != \"data\" and applyMuHnlPtCorr:\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        auto hnl_pt_shape_sf_file = TFile::Open(\"{}\");\n",
    "        auto h_hnl_pt_shape_sf = hnl_pt_shape_sf_file->Get<TH1D>(\"{}\");\n",
    "        \"\"\".format(config[\"hnl_pt_shape_sf_input_file\"],config[\"hnl_pt_shape_sf_histo_name\"])\n",
    "        )\n",
    "    #get mu_Ds IPS shape scale factors histogram\n",
    "    if dataset_category != \"data\" and applyMuDsIPSCorr:\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        auto ds_ips_shape_sf_file = TFile::Open(\"{}\");\n",
    "        auto h_ds_ips_shape_sf = ds_ips_shape_sf_file->Get<TH1D>(\"{}\");\n",
    "        \"\"\".format(config[\"ds_ips_shape_sf_input_file\"],config[\"ds_ips_shape_sf_histo_name\"])\n",
    "        )\n",
    "    #get mu_Hnl IPS shape scale factors histogram\n",
    "    if dataset_category != \"data\" and applyMuHnlIPSCorr:\n",
    "        ROOT.gInterpreter.Declare(\"\"\"\n",
    "        auto hnl_ips_shape_sf_file = TFile::Open(\"{}\");\n",
    "        auto h_hnl_ips_shape_sf = hnl_ips_shape_sf_file->Get<TH1D>(\"{}\");\n",
    "        \"\"\".format(config[\"hnl_ips_shape_sf_input_file\"],config[\"hnl_ips_shape_sf_histo_name\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f85ae-5c8e-4af1-903c-2ff3e3a5bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ade54-c850-4041-a961-1066a34b37b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Continue with the analysis flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3b51c-69ae-4a97-9265-17ba61618976",
   "metadata": {},
   "source": [
    "Print information on the auxiliary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627d171-2ee1-4bfd-9be9-12318b8e1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_category != \"data\" and not skipTrigSF:\n",
    "    print(\"eff data histogram from {}, {}\".format(config[\"trigger_eff_data_input_file\"],config[\"trigger_eff_data_histo_name\"]))\n",
    "    print(\"eff mc histogram from {}, {}\".format(config[\"trigger_eff_mc_input_file\"],config[\"trigger_eff_mc_histo_name\"]))\n",
    "if dataset_category != \"data\" and applyMuDsPtCorr:\n",
    "    print(\"pt shape correction sf from {}, {}\".format(config[\"ds_pt_shape_sf_input_file\"],config[\"ds_pt_shape_sf_histo_name\"]))\n",
    "if dataset_category != \"data\" and applyMuHnlPtCorr:\n",
    "    print(\"pt shape correction sf from {}, {}\".format(config[\"hnl_pt_shape_sf_input_file\"],config[\"hnl_pt_shape_sf_histo_name\"]))\n",
    "if dataset_category != \"data\" and applyMuDsIPSCorr:\n",
    "    print(\"ips shape correction sf from {}, {}\".format(config[\"ds_ips_shape_sf_input_file\"],config[\"ds_ips_shape_sf_histo_name\"]))\n",
    "if dataset_category != \"data\" and applyMuHnlIPSCorr:\n",
    "    print(\"ips shape correction sf from {}, {}\".format(config[\"hnl_ips_shape_sf_input_file\"],config[\"hnl_ips_shape_sf_histo_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a7270-7be4-4dd7-8ba3-810b55c60f8d",
   "metadata": {},
   "source": [
    "Define unit weights and generator weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dea393-97ea-4246-8c1c-7737f08e940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define unit weights\n",
    "mc_weight  = 1.\n",
    "pu_weight  = 1.\n",
    "\n",
    "# get generator weight for MC\n",
    "if dataset_category != \"data\":\n",
    "    cross_section      = float(ntuples[dataset_to_process][\"cross_section\"])\n",
    "    filter_efficiency  = float(ntuples[dataset_to_process][\"filter_efficiency\"])\n",
    "    total_events       = float(ntuples[dataset_to_process][\"processed_events\"])\n",
    "    mc_weight = cross_section*filter_efficiency/total_events\n",
    "\n",
    "print(\"mc_weight: {}\".format(mc_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8963dcc-ac53-4f3f-b734-06d04ffe67c6",
   "metadata": {},
   "source": [
    "List of columns selected for the analysis as well as entire column list from the ntuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1c8bf-949a-484c-b17b-b79f0d827c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ListOfColumns = { \"C_Ds_mass\", \"C_Ds_pt\", \"C_Ds_px\", \"C_Ds_py\", \"C_Ds_pz\", \"C_Ds_vertex_2DDist_BS\", \n",
    "                 \"C_Ds_vertex_2DDist_PV\", \"C_Ds_vertex_2DErr_BS\", \"C_Ds_vertex_2DErr_PV\", \"C_Ds_vertex_2DSig_BS\", \n",
    "                 \"C_Ds_vertex_2DSig_PV\", \"C_Ds_vertex_3DDist_BS\", \"C_Ds_vertex_3DDist_PV\", \"C_Ds_vertex_3DErr_BS\", \n",
    "                 \"C_Ds_vertex_3DErr_PV\", \"C_Ds_vertex_3DSig_BS\", \"C_Ds_vertex_3DSig_PV\", \"C_Ds_vertex_cos2D\", \n",
    "                 \"C_Ds_vertex_cos3D\", \"C_Ds_vertex_prob\", \"C_Ds_vertex_x\", \"C_Ds_vertex_xErr\", \"C_Ds_vertex_y\", \n",
    "                 \"C_Ds_vertex_yErr\", \"C_Ds_vertex_z\", \"C_Ds_vertex_zErr\", \"C_Hnl_gen_l\", \"C_Hnl_gen_l_prop\", \n",
    "                 \"C_Hnl_l_prop\", \"C_Hnl_mass\", \"C_Hnl_p\", \"C_Hnl_pt\", \"C_Hnl_px\", \"C_Hnl_py\", \"C_Hnl_pz\", \n",
    "                 \"C_Hnl_vertex_2DDist_BS\", \"C_Hnl_vertex_2DDist_PV\", \"C_Hnl_vertex_2DErr_BS\", \"C_Hnl_vertex_2DErr_PV\", \n",
    "                 \"C_Hnl_vertex_2DSig_BS\", \"C_Hnl_vertex_2DSig_PV\", \"C_Hnl_vertex_3DDist_BS\", \"C_Hnl_vertex_3DDist_PV\", \n",
    "                 \"C_Hnl_vertex_3DErr_BS\", \"C_Hnl_vertex_3DErr_PV\", \"C_Hnl_vertex_3DSig_BS\", \"C_Hnl_vertex_3DSig_PV\", \n",
    "                 \"C_Hnl_vertex_cos2D\", \"C_Hnl_vertex_cos3D\", \"C_Hnl_vertex_prob\", \"C_Hnl_vertex_x\", \"C_Hnl_vertex_xErr\", \n",
    "                 \"C_Hnl_vertex_y\", \"C_Hnl_vertex_yErr\", \"C_Hnl_vertex_z\", \"C_Hnl_vertex_zErr\", \"C_mu1mu2_dr\", \n",
    "                 \"C_mu1mu2_mass\", \"C_mu1pi_dr\", \"C_mu2pi_dr\", \"C_mu_Ds_BS_ip\", \"C_mu_Ds_BS_ip_xy\", \"C_mu_Ds_BS_ips\", \n",
    "                 \"C_mu_Ds_BS_ips_xy\", \"C_mu_Ds_BS_x\", \"C_mu_Ds_BS_y\", \"C_mu_Ds_BS_z\", \"C_mu_Ds_PV_ip\", \"C_mu_Ds_PV_ip_xy\", \n",
    "                 \"C_mu_Ds_PV_ip_z\", \"C_mu_Ds_PV_ips\", \"C_mu_Ds_PV_ips_xy\", \"C_mu_Ds_PV_ips_z\", \"C_mu_Ds_charge\", \n",
    "                 \"C_mu_Ds_dptopt_MU10p5_IP3p5\", \"C_mu_Ds_dptopt_MU12_IP6\", \"C_mu_Ds_dptopt_MU7_IP4\", \"C_mu_Ds_dptopt_MU8_IP3\", \n",
    "                 \"C_mu_Ds_dptopt_MU8_IP5\", \"C_mu_Ds_dptopt_MU8_IP6\", \"C_mu_Ds_dptopt_MU8p5_IP3p5\", \"C_mu_Ds_dptopt_MU9_IP4\", \n",
    "                 \"C_mu_Ds_dptopt_MU9_IP5\", \"C_mu_Ds_dptopt_MU9_IP6\", \"C_mu_Ds_dr_MU10p5_IP3p5\", \"C_mu_Ds_dr_MU12_IP6\", \n",
    "                 \"C_mu_Ds_dr_MU7_IP4\", \"C_mu_Ds_dr_MU8_IP3\", \"C_mu_Ds_dr_MU8_IP5\", \"C_mu_Ds_dr_MU8_IP6\", \"C_mu_Ds_dr_MU8p5_IP3p5\", \n",
    "                 \"C_mu_Ds_dr_MU9_IP4\", \"C_mu_Ds_dr_MU9_IP5\", \"C_mu_Ds_dr_MU9_IP6\", \"C_mu_Ds_eta\", \"C_mu_Ds_fitted_E\", \n",
    "                 \"C_mu_Ds_fitted_px\", \"C_mu_Ds_fitted_py\", \"C_mu_Ds_fitted_pz\", \"C_mu_Ds_iBestPV\",\n",
    "                 \"C_mu_Ds_idx\", \"C_mu_Ds_isGlobal\", \"C_mu_Ds_isHnlBrother\", \"C_mu_Ds_isLoose\", \"C_mu_Ds_isMedium\", \n",
    "                 \"C_mu_Ds_isSoft\", \"C_mu_Ds_isStandAlone\", \"C_mu_Ds_isTracker\", \"C_mu_Ds_matched_MU10p5_IP3p5\",\n",
    "                 \"C_mu_Ds_matched_MU12_IP6\", \"C_mu_Ds_matched_MU7_IP4\", \"C_mu_Ds_matched_MU8_IP3\", \"C_mu_Ds_matched_MU8_IP5\", \n",
    "                 \"C_mu_Ds_matched_MU8_IP6\", \"C_mu_Ds_matched_MU8p5_IP3p5\", \"C_mu_Ds_matched_MU9_IP4\", \"C_mu_Ds_matched_MU9_IP5\", \n",
    "                 \"C_mu_Ds_matched_MU9_IP6\", \"C_mu_Ds_phi\", \"C_mu_Ds_pt\", \"C_mu_Hnl_BS_ip\", \"C_mu_Hnl_BS_ip_xy\",\n",
    "                 \"C_mu_Hnl_BS_ips\", \"C_mu_Hnl_BS_ips_xy\", \"C_mu_Hnl_PV_ip\", \"C_mu_Hnl_PV_ip_xy\", \"C_mu_Hnl_PV_ip_z\", \n",
    "                 \"C_mu_Hnl_PV_ips\", \"C_mu_Hnl_PV_ips_xy\", \"C_mu_Hnl_PV_ips_z\", \"C_mu_Hnl_charge\", \"C_mu_Hnl_dptopt_MU10p5_IP3p5\", \n",
    "                 \"C_mu_Hnl_dptopt_MU12_IP6\", \"C_mu_Hnl_dptopt_MU7_IP4\", \"C_mu_Hnl_dptopt_MU8_IP3\", \"C_mu_Hnl_dptopt_MU8_IP5\", \n",
    "                 \"C_mu_Hnl_dptopt_MU8_IP6\", \"C_mu_Hnl_dptopt_MU8p5_IP3p5\", \"C_mu_Hnl_dptopt_MU9_IP4\", \"C_mu_Hnl_dptopt_MU9_IP5\", \n",
    "                 \"C_mu_Hnl_dptopt_MU9_IP6\", \"C_mu_Hnl_dr_MU10p5_IP3p5\", \"C_mu_Hnl_dr_MU12_IP6\", \"C_mu_Hnl_dr_MU7_IP4\", \n",
    "                 \"C_mu_Hnl_dr_MU8_IP3\", \"C_mu_Hnl_dr_MU8_IP5\", \"C_mu_Hnl_dr_MU8_IP6\", \"C_mu_Hnl_dr_MU8p5_IP3p5\", \n",
    "                 \"C_mu_Hnl_dr_MU9_IP4\", \"C_mu_Hnl_dr_MU9_IP5\", \"C_mu_Hnl_dr_MU9_IP6\", \"C_mu_Hnl_eta\", \"C_mu_Hnl_fitted_E\", \n",
    "                 \"C_mu_Hnl_fitted_px\", \"C_mu_Hnl_fitted_py\", \"C_mu_Hnl_fitted_pz\", \"C_mu_Hnl_idx\",\n",
    "                 \"C_mu_Hnl_isGlobal\", \"C_mu_Hnl_isHnlDaughter\", \"C_mu_Hnl_isLoose\", \"C_mu_Hnl_isMedium\", \"C_mu_Hnl_isSoft\", \n",
    "                 \"C_mu_Hnl_isStandAlone\", \"C_mu_Hnl_isTracker\", \"C_mu_Hnl_matched_MU10p5_IP3p5\",\n",
    "                 \"C_mu_Hnl_matched_MU12_IP6\", \"C_mu_Hnl_matched_MU7_IP4\", \"C_mu_Hnl_matched_MU8_IP3\", \"C_mu_Hnl_matched_MU8_IP5\", \n",
    "                 \"C_mu_Hnl_matched_MU8_IP6\", \"C_mu_Hnl_matched_MU8p5_IP3p5\", \"C_mu_Hnl_matched_MU9_IP4\", \"C_mu_Hnl_matched_MU9_IP5\", \n",
    "                 \"C_mu_Hnl_matched_MU9_IP6\", \"C_mu_Hnl_phi\", \"C_mu_Hnl_pt\", \"C_pi_BS_ip\",\n",
    "                 \"C_pi_BS_ip_xy\", \"C_pi_BS_ip_z\", \"C_pi_BS_ips\", \"C_pi_BS_ips_xy\", \"C_pi_BS_ips_z\", \"C_pi_PV_ip_xy\", \"C_pi_PV_ip_z\", \n",
    "                 \"C_pi_PV_ips_xy\", \"C_pi_PV_ips_z\", \"C_pi_charge\", \"C_pi_eta\", \"C_pi_fitted_E\", \"C_pi_fitted_px\", \"C_pi_fitted_py\", \n",
    "                 \"C_pi_fitted_pz\", \"C_pi_idx\", \"C_pi_isHnlDaughter\", \"C_pi_phi\", \"C_pi_pt\", \"HLT_mu10p5_ip3p5_dr\",\n",
    "                 \"HLT_mu10p5_ip3p5_eta\", \"HLT_mu10p5_ip3p5_matched\", \"HLT_mu10p5_ip3p5_prescale\", \"HLT_mu10p5_ip3p5_pt\", \"HLT_mu12_ip6_dr\", \n",
    "                 \"HLT_mu12_ip6_eta\", \"HLT_mu12_ip6_matched\", \"HLT_mu12_ip6_prescale\", \"HLT_mu12_ip6_pt\", \"HLT_mu7_ip4_dr\", \n",
    "                 \"HLT_mu7_ip4_eta\", \"HLT_mu7_ip4_matched\", \"HLT_mu7_ip4_prescale\", \"HLT_mu7_ip4_pt\", \"HLT_mu8_ip3_dr\", \"HLT_mu8_ip3_eta\", \n",
    "                 \"HLT_mu8_ip3_matched\", \"HLT_mu8_ip3_prescale\", \"HLT_mu8_ip3_pt\", \"HLT_mu8_ip5_dr\", \"HLT_mu8_ip5_eta\", \n",
    "                 \"HLT_mu8_ip5_matched\", \"HLT_mu8_ip5_prescale\", \"HLT_mu8_ip5_pt\", \"HLT_mu8_ip6_dr\", \"HLT_mu8_ip6_eta\", \"HLT_mu8_ip6_matched\", \n",
    "                 \"HLT_mu8_ip6_prescale\", \"HLT_mu8_ip6_pt\", \"HLT_mu8p5_ip3p5_dr\", \"HLT_mu8p5_ip3p5_eta\", \"HLT_mu8p5_ip3p5_matched\", \n",
    "                 \"HLT_mu8p5_ip3p5_prescale\", \"HLT_mu8p5_ip3p5_pt\", \"HLT_mu9_ip4_dr\", \"HLT_mu9_ip4_eta\", \"HLT_mu9_ip4_matched\", \n",
    "                 \"HLT_mu9_ip4_prescale\", \"HLT_mu9_ip4_pt\", \"HLT_mu9_ip5_dr\", \"HLT_mu9_ip5_eta\", \"HLT_mu9_ip5_matched\", \"HLT_mu9_ip5_prescale\", \n",
    "                 \"HLT_mu9_ip5_pt\", \"HLT_mu9_ip6_dr\", \"HLT_mu9_ip6_eta\", \"HLT_mu9_ip6_matched\", \"HLT_mu9_ip6_prescale\", \"HLT_mu9_ip6_pt\", \n",
    "                 \"HLT_mu_trig_eta\", \"HLT_mu_trig_idx\", \"HLT_mu_trig_phi\", \"HLT_mu_trig_pt\", \"PV_prob\", \"PV_x\", \"PV_xErr\", \"PV_y\", \"PV_yErr\", \n",
    "                 \"PV_z\", \"PV_zErr\", \"event\", \"lumi\", \"mc_weight\", \"nCand\", \"nPU_trueInt\", \"nPV\", \"numTrack\", \"pass_presel_0\", \"pass_presel_1\", \n",
    "                 \"pass_presel_2\", \"pu_weight\", \"run\", \"tot_weight\" }\n",
    "\n",
    "good_cols = {'C_Ds_mass', 'C_Ds_pt', 'C_Ds_px', 'C_Ds_py', 'C_Ds_pz', 'C_Ds_vertex_2DDist_BS', 'C_Ds_vertex_2DDist_PV', 'C_Ds_vertex_2DErr_BS', \n",
    "             'C_Ds_vertex_2DErr_PV', 'C_Ds_vertex_2DSig_BS', 'C_Ds_vertex_2DSig_PV', 'C_Ds_vertex_3DDist_BS', 'C_Ds_vertex_3DDist_PV', \n",
    "             'C_Ds_vertex_3DErr_BS', 'C_Ds_vertex_3DErr_PV', 'C_Ds_vertex_3DSig_BS', 'C_Ds_vertex_3DSig_PV', 'C_Ds_vertex_cos2D', \n",
    "             'C_Ds_vertex_cos3D', 'C_Ds_vertex_prob', 'C_Ds_vertex_x', 'C_Ds_vertex_xErr', 'C_Ds_vertex_y', 'C_Ds_vertex_yErr', \n",
    "             'C_Ds_vertex_z', 'C_Ds_vertex_zErr', 'C_Hnl_gen_l', 'C_Hnl_gen_l_prop', 'C_Hnl_l_prop', 'C_Hnl_mass', 'C_Hnl_p', 'C_Hnl_pt', \n",
    "             'C_Hnl_px', 'C_Hnl_py', 'C_Hnl_pz', 'C_Hnl_vertex_2DDist_BS', 'C_Hnl_vertex_2DDist_PV', 'C_Hnl_vertex_2DErr_BS', \n",
    "             'C_Hnl_vertex_2DErr_PV', 'C_Hnl_vertex_2DSig_BS', 'C_Hnl_vertex_2DSig_PV', 'C_Hnl_vertex_3DDist_BS', 'C_Hnl_vertex_3DDist_PV', \n",
    "             'C_Hnl_vertex_3DErr_BS', 'C_Hnl_vertex_3DErr_PV', 'C_Hnl_vertex_3DSig_BS', 'C_Hnl_vertex_3DSig_PV', 'C_Hnl_vertex_cos2D', \n",
    "             'C_Hnl_vertex_cos3D', 'C_Hnl_vertex_prob', 'C_Hnl_vertex_x', 'C_Hnl_vertex_xErr', 'C_Hnl_vertex_y', 'C_Hnl_vertex_yErr', \n",
    "             'C_Hnl_vertex_z', 'C_Hnl_vertex_zErr', 'C_mu1mu2_dr', 'C_mu1mu2_mass', 'C_mu1pi_dr', 'C_mu2pi_dr', 'C_mu_Ds_BS_ip_xy', \n",
    "             'C_mu_Ds_BS_ips_xy', 'C_mu_Ds_BS_x', 'C_mu_Ds_BS_y', 'C_mu_Ds_BS_z', 'C_mu_Ds_PV_ip_xy', 'C_mu_Ds_PV_ip_z', 'C_mu_Ds_PV_ips_xy', \n",
    "             'C_mu_Ds_PV_ips_z', 'C_mu_Ds_charge', 'C_mu_Ds_dptopt_MU10p5_IP3p5', 'C_mu_Ds_dptopt_MU12_IP6', 'C_mu_Ds_dptopt_MU7_IP4', \n",
    "             'C_mu_Ds_dptopt_MU8_IP3', 'C_mu_Ds_dptopt_MU8_IP5', 'C_mu_Ds_dptopt_MU8_IP6', 'C_mu_Ds_dptopt_MU8p5_IP3p5', 'C_mu_Ds_dptopt_MU9_IP4', \n",
    "             'C_mu_Ds_dptopt_MU9_IP5', 'C_mu_Ds_dptopt_MU9_IP6', 'C_mu_Ds_dr_MU10p5_IP3p5', 'C_mu_Ds_dr_MU12_IP6', 'C_mu_Ds_dr_MU7_IP4', \n",
    "             'C_mu_Ds_dr_MU8_IP3', 'C_mu_Ds_dr_MU8_IP5', 'C_mu_Ds_dr_MU8_IP6', 'C_mu_Ds_dr_MU8p5_IP3p5', 'C_mu_Ds_dr_MU9_IP4', \n",
    "             'C_mu_Ds_dr_MU9_IP5', 'C_mu_Ds_dr_MU9_IP6', 'C_mu_Ds_eta', 'C_mu_Ds_fitted_E', 'C_mu_Ds_fitted_px', 'C_mu_Ds_fitted_py', \n",
    "             'C_mu_Ds_fitted_pz', 'C_mu_Ds_iBestPV', 'C_mu_Ds_idx', 'C_mu_Ds_isGlobal', 'C_mu_Ds_isHnlBrother',\n",
    "             'C_mu_Ds_isLoose', 'C_mu_Ds_isMedium', 'C_mu_Ds_isSoft', 'C_mu_Ds_isStandAlone', 'C_mu_Ds_isTracker', \n",
    "             'C_mu_Ds_matched_MU10p5_IP3p5', 'C_mu_Ds_matched_MU12_IP6', 'C_mu_Ds_matched_MU7_IP4', 'C_mu_Ds_matched_MU8_IP3', \n",
    "             'C_mu_Ds_matched_MU8_IP5', 'C_mu_Ds_matched_MU8_IP6', 'C_mu_Ds_matched_MU8p5_IP3p5', 'C_mu_Ds_matched_MU9_IP4', \n",
    "             'C_mu_Ds_matched_MU9_IP5', 'C_mu_Ds_matched_MU9_IP6', 'C_mu_Ds_phi', 'C_mu_Ds_pt', 'C_mu_Hnl_BS_ip_xy',\n",
    "             'C_mu_Hnl_BS_ips_xy', 'C_mu_Hnl_PV_ip_xy', 'C_mu_Hnl_PV_ip_z', 'C_mu_Hnl_PV_ips_xy', 'C_mu_Hnl_PV_ips_z', 'C_mu_Hnl_charge', \n",
    "             'C_mu_Hnl_dptopt_MU10p5_IP3p5', 'C_mu_Hnl_dptopt_MU12_IP6', 'C_mu_Hnl_dptopt_MU7_IP4', 'C_mu_Hnl_dptopt_MU8_IP3', \n",
    "             'C_mu_Hnl_dptopt_MU8_IP5', 'C_mu_Hnl_dptopt_MU8_IP6', 'C_mu_Hnl_dptopt_MU8p5_IP3p5', 'C_mu_Hnl_dptopt_MU9_IP4', \n",
    "             'C_mu_Hnl_dptopt_MU9_IP5', 'C_mu_Hnl_dptopt_MU9_IP6', 'C_mu_Hnl_dr_MU10p5_IP3p5', 'C_mu_Hnl_dr_MU12_IP6', 'C_mu_Hnl_dr_MU7_IP4', \n",
    "             'C_mu_Hnl_dr_MU8_IP3', 'C_mu_Hnl_dr_MU8_IP5', 'C_mu_Hnl_dr_MU8_IP6', 'C_mu_Hnl_dr_MU8p5_IP3p5', 'C_mu_Hnl_dr_MU9_IP4', \n",
    "             'C_mu_Hnl_dr_MU9_IP5', 'C_mu_Hnl_dr_MU9_IP6', 'C_mu_Hnl_eta', 'C_mu_Hnl_fitted_E', 'C_mu_Hnl_fitted_px', 'C_mu_Hnl_fitted_py', \n",
    "             'C_mu_Hnl_fitted_pz' , 'C_mu_Hnl_idx', 'C_mu_Hnl_isGlobal', 'C_mu_Hnl_isHnlDaughter', 'C_mu_Hnl_isLoose',\n",
    "             'C_mu_Hnl_isMedium', 'C_mu_Hnl_isSoft', 'C_mu_Hnl_isStandAlone', 'C_mu_Hnl_isTracker', \n",
    "             'C_mu_Hnl_matched_MU10p5_IP3p5', 'C_mu_Hnl_matched_MU12_IP6', 'C_mu_Hnl_matched_MU7_IP4', 'C_mu_Hnl_matched_MU8_IP3', \n",
    "             'C_mu_Hnl_matched_MU8_IP5', 'C_mu_Hnl_matched_MU8_IP6', 'C_mu_Hnl_matched_MU8p5_IP3p5', 'C_mu_Hnl_matched_MU9_IP4', \n",
    "             'C_mu_Hnl_matched_MU9_IP5', 'C_mu_Hnl_matched_MU9_IP6', 'C_mu_Hnl_phi', 'C_mu_Hnl_pt',\n",
    "             'C_pi_BS_ip', 'C_pi_BS_ip_xy', 'C_pi_BS_ip_z', 'C_pi_BS_ips', 'C_pi_BS_ips_xy', 'C_pi_BS_ips_z', 'C_pi_PV_ip_xy', \n",
    "             'C_pi_PV_ip_z', 'C_pi_PV_ips_xy', 'C_pi_PV_ips_z', 'C_pi_charge', 'C_pi_eta', 'C_pi_fitted_E', 'C_pi_fitted_px', 'C_pi_fitted_py', \n",
    "             'C_pi_fitted_pz', 'C_pi_idx', 'C_pi_isHnlDaughter', 'C_pi_phi', 'C_pi_pt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753de5a9-8fba-4fc0-a6e4-2cc380bbee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    numWorkers= len(client.scheduler_info()['workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca492054-7f6a-42ab-9cc7-78d51963c089",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DataFrame creation and execution on Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = inputFileName_list[0].split(\"/\")[-1].split(\".\")[0]\n",
    "dataset_name_label = input_file_name[input_file_name.find(\"_\")+1:input_file_name.rfind(\"_\")]\n",
    "\n",
    "######################\n",
    "###### ANALYSIS ######\n",
    "######################\n",
    "pre_stop = time.time()\n",
    "if distributed:\n",
    "    client.run(clear_nodes, with_aux_files=False)\n",
    "    \n",
    "#initialize data frame\n",
    "if distributed:\n",
    "    df = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame(tree_name, inputFileName_list, npartitions=3*numWorkers, daskclient=client, monitor_label = \"HNL\") #len(inputFileName_list)\n",
    "else:\n",
    "    df = ROOT.RDataFrame(tree_name, inputFileName_list)\n",
    "\n",
    "nEvents = df.Count()\n",
    "#define new variables\n",
    "for var in selection[\"new_variables\"]:\n",
    "    df = df.Define(var[\"name\"],var[\"definition\"])\n",
    "\n",
    "# operations on signal samples\n",
    "if dataset_category == \"signal\":\n",
    "    #define GEN matching variables and reject events w/o GEN matched candidates\n",
    "    gen_matching_cuts =list()\n",
    "    mask_var = \"C_pass_gen_matching\"\n",
    "    ListOfColumns.add(mask_var)\n",
    "    good_cols.add(mask_var)\n",
    "    for sel in selection[\"gen_matching_cuts\"]:\n",
    "        gen_matching_cuts.append(sel[\"cut\"])\n",
    "    df = df.Define(mask_var,\" && \".join(gen_matching_cuts))\n",
    "    df = df.Filter(\"ROOT::VecOps::Any(\"+mask_var+\")\",\"pass GEN matching cuts\")\n",
    "\n",
    "    # define ctau weights\n",
    "    if ctauReweighting and dataset_category == \"signal\":\n",
    "        old_ctau_label = dataset_name_label[dataset_name_label.find(\"ctau\")+4:dataset_name_label.find(\"mm\")]\n",
    "        hnl_mass_label = dataset_name_label[dataset_name_label.find(\"mN\")+2:dataset_name_label.find(\"_ctau\")]\n",
    "        for new_ctau in selection[\"mN\"+hnl_mass_label+\"_ctau\"+old_ctau_label+\"mm_rw_points\"]:\n",
    "          old_ctau = float(old_ctau_label.replace(\"p\",\".\"))\n",
    "          w_expr   = \"(\"+str(old_ctau)+\"/\"+str(new_ctau)+\")*\"+\"exp(C_Hnl_gen_l_prop*(\"+str(1./old_ctau)+\"-\"+str(1./new_ctau)+\"))\"\n",
    "          df = df.Define(\"C_ctau_weight_\"+old_ctau_label+\"TO\"+str(new_ctau).replace(\".\",\"p\"),w_expr)\n",
    "\n",
    "\n",
    "#################\n",
    "#### WEIGHTS ####\n",
    "#################\n",
    "\n",
    "#define cross section normalization mc_weight\n",
    "df = df.Define(\"mc_weight\",str(mc_weight))    \n",
    "\n",
    "# define pu weight for MC only\n",
    "if dataset_category != \"data\" and not skipPUrw:\n",
    "    pu_weight = \"h_pu_weights->GetBinContent(h_pu_weights->FindBin(nPU_trueInt))\"\n",
    "df = df.Define(\"pu_weight\",str(pu_weight)) \n",
    "df = df.Define(\"tot_weight\",\"mc_weight*pu_weight\")\n",
    "\n",
    "# define trigger scale factors for MC only\n",
    "if dataset_category != \"data\" and not skipTrigSF:\n",
    "    trigger_eff_data_ds  = \"get_2D_binContent(h_trigger_eff_data,C_{mu1l}_pt,C_{mu1l}_BS_ips_xy,99.0,499.0)\".format(mu1l=config[\"mu1_label\"])\n",
    "    trigger_eff_mc_ds    = \"get_2D_binContent(h_trigger_eff_mc,C_{mu1l}_pt,C_{mu1l}_BS_ips_xy,99.0,499.0)\".format(mu1l=config[\"mu1_label\"])\n",
    "    trigger_eff_data_hnl = \"get_2D_binContent(h_trigger_eff_data,C_{mu2l}_pt,C_{mu2l}_BS_ips_xy,99.0,499.0)\".format(mu2l=config[\"mu2_label\"])\n",
    "    trigger_eff_mc_hnl   = \"get_2D_binContent(h_trigger_eff_mc,C_{mu2l}_pt,C_{mu2l}_BS_ips_xy,99.0,499.0)\".format(mu2l=config[\"mu2_label\"])\n",
    "    trigger_sf_columns = {\"C_mu_Ds_matched_HLT\", \"C_mu_Hnl_matched_HLT\",\"C_trigger_eff_mc_ds\", \"C_trigger_sf\", \"C_trigger_eff_mc_hnl\",\"C_trigger_eff_data_hnl\", \"C_trigger_eff_data_ds\"}\n",
    "    ListOfColumns.update(trigger_sf_columns)\n",
    "    good_cols.update(trigger_sf_columns)\n",
    "    df = df.Define(\"C_trigger_eff_data_ds\",str(trigger_eff_data_ds)) \n",
    "    df = df.Define(\"C_trigger_eff_data_hnl\",str(trigger_eff_data_hnl)) \n",
    "    df = df.Define(\"C_trigger_eff_mc_ds\",str(trigger_eff_mc_ds))\n",
    "    df = df.Define(\"C_trigger_eff_mc_hnl\",str(trigger_eff_mc_hnl)) \n",
    "    df = df.Define(\"C_{mu2l}_matched_HLT\".format(mu2l=config[\"mu2_label\"]),\"(C_{mu2l}_matched_MU7_IP4>0 && C_{mu2l}_dr_MU7_IP4<0.05 && C_{mu2l}_dptopt_MU7_IP4<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU8_IP3>0 && C_{mu2l}_dr_MU8_IP3<0.05 && C_{mu2l}_dptopt_MU8_IP3<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU8_IP5>0 && C_{mu2l}_dr_MU8_IP5<0.05 && C_{mu2l}_dptopt_MU8_IP5<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU8_IP6>0 && C_{mu2l}_dr_MU8_IP6<0.05 && C_{mu2l}_dptopt_MU8_IP6<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU9_IP4>0 && C_{mu2l}_dr_MU9_IP4<0.05 && C_{mu2l}_dptopt_MU9_IP4<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU9_IP5>0 && C_{mu2l}_dr_MU9_IP5<0.05 && C_{mu2l}_dptopt_MU9_IP5<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU9_IP6>0 && C_{mu2l}_dr_MU9_IP6<0.05 && C_{mu2l}_dptopt_MU9_IP6<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU10p5_IP3p5>0 && C_{mu2l}_dr_MU10p5_IP3p5<0.05 && C_{mu2l}_dptopt_MU10p5_IP3p5<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45) || (C_{mu2l}_matched_MU12_IP6>0 && C_{mu2l}_dr_MU12_IP6<0.05 && C_{mu2l}_dptopt_MU12_IP6<0.1 && C_{mu2l}_pt >7.5 && C_{mu2l}_eta>-1.45 && C_{mu2l}_eta<1.45)\".format(mu2l=config[\"mu2_label\"])) \n",
    "    df = df.Define(\"C_{mu1l}_matched_HLT\".format(mu1l=config[\"mu1_label\"]),\"(C_{mu1l}_matched_MU7_IP4>0 && C_{mu1l}_dr_MU7_IP4<0.05 && C_{mu1l}_dptopt_MU7_IP4<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU8_IP3>0 && C_{mu1l}_dr_MU8_IP3<0.05 && C_{mu1l}_dptopt_MU8_IP3<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU8_IP5>0 && C_{mu1l}_dr_MU8_IP5<0.05 && C_{mu1l}_dptopt_MU8_IP5<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU8_IP6>0 && C_{mu1l}_dr_MU8_IP6<0.05 && C_{mu1l}_dptopt_MU8_IP6<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU9_IP4>0 && C_{mu1l}_dr_MU9_IP4<0.05 && C_{mu1l}_dptopt_MU9_IP4<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU9_IP5>0 && C_{mu1l}_dr_MU9_IP5<0.05 && C_{mu1l}_dptopt_MU9_IP5<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU9_IP6>0 && C_{mu1l}_dr_MU9_IP6<0.05 && C_{mu1l}_dptopt_MU9_IP6<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU10p5_IP3p5>0 && C_{mu1l}_dr_MU10p5_IP3p5<0.05 && C_{mu1l}_dptopt_MU10p5_IP3p5<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45) || (C_{mu1l}_matched_MU12_IP6>0 && C_{mu1l}_dr_MU12_IP6<0.05 && C_{mu1l}_dptopt_MU12_IP6<0.1 && C_{mu1l}_pt >7.5 && C_{mu1l}_eta>-1.45 && C_{mu1l}_eta<1.45)\".format(mu1l=config[\"mu1_label\"])) \n",
    "    df = df.Define(\"C_trigger_sf\",\"vcompute_total_sf(C_trigger_eff_data_ds,C_trigger_eff_mc_ds,C_{mu1l}_matched_HLT,C_trigger_eff_data_hnl,C_trigger_eff_mc_hnl,C_{mu2l}_matched_HLT)\".format(mu1l=config[\"mu1_label\"],mu2l=config[\"mu2_label\"]))\n",
    "\n",
    "# define mu id factors for MC only\n",
    "if dataset_category != \"data\" and not skipMuIDsf:\n",
    "    mu_id_sf_columns = {\"C_mu_Ds_id_sf\",\"C_mu_Hnl_id_sf\"}\n",
    "    ListOfColumns.update(mu_id_sf_columns)\n",
    "    good_cols.update(mu_id_sf_columns)\n",
    "    variation = varyMuIDSf\n",
    "    mu1_id_sf = \"vget_mu_id_sf(mu_id_sf_cfg,C_{mu1l}_pt,C_{mu1l}_eta,{variation})\".format(mu1l=config[\"mu1_label\"],variation=variation)\n",
    "    mu2_id_sf = \"vget_mu_id_sf(mu_id_sf_cfg,C_{mu2l}_pt,C_{mu2l}_eta,{variation})\".format(mu2l=config[\"mu2_label\"],variation=variation)\n",
    "    df = df.Define(\"C_{mu1l}_id_sf\".format(mu1l=config[\"mu1_label\"]),mu1_id_sf)\n",
    "    df = df.Define(\"C_{mu2l}_id_sf\".format(mu2l=config[\"mu2_label\"]),mu2_id_sf)\n",
    "\n",
    "# define mu reco factors for MC only\n",
    "if dataset_category != \"data\" and not skipMuRecosf:\n",
    "    mu_reco_sf_columns = {\"C_mu_Ds_reco_sf\",\"C_mu_Hnl_reco_sf\"}\n",
    "    ListOfColumns.update(mu_reco_sf_columns)\n",
    "    good_cols.update(mu_reco_sf_columns)\n",
    "    variation = varyMuRecoSf\n",
    "    mu1_reco_sf = \"vget_mu_reco_sf(mu_reco_sf_cfg,C_{mu1l}_pt,C_{mu1l}_eta,{variation})\".format(mu1l=config[\"mu1_label\"],variation=variation)\n",
    "    mu2_reco_sf = \"vget_mu_reco_sf(mu_reco_sf_cfg,C_{mu2l}_pt,C_{mu2l}_eta,{variation})\".format(mu2l=config[\"mu2_label\"],variation=variation)\n",
    "    df = df.Define(\"C_{mu1l}_reco_sf\".format(mu1l=config[\"mu1_label\"]),mu1_reco_sf) \n",
    "    df = df.Define(\"C_{mu2l}_reco_sf\".format(mu2l=config[\"mu2_label\"]),mu2_reco_sf) \n",
    "\n",
    "# define mu_Ds pt shape scale factors for MC only\n",
    "if dataset_category != \"data\" and applyMuDsPtCorr:\n",
    "    ds_pt_shape_sf  = \"get_1D_binContent(h_ds_pt_shape_sf,C_{}_pt)\".format(config[\"mu1_label\"])\n",
    "    df = df.Define(\"C_ds_pt_shape_sf\",str(ds_pt_shape_sf)) \n",
    "\n",
    "# define mu_Hnl pt shape scale factors for MC only\n",
    "if dataset_category != \"data\" and applyMuHnlPtCorr:\n",
    "    hnl_pt_shape_sf  = \"get_1D_binContent(h_hnl_pt_shape_sf,C_{}_pt)\".format(config[\"mu2_label\"])\n",
    "    df = df.Define(\"C_hnl_pt_shape_sf\",str(hnl_pt_shape_sf)) \n",
    "\n",
    "# define mu_Ds IPS shape scale factors for MC only\n",
    "if dataset_category != \"data\" and applyMuDsIPSCorr:\n",
    "    ds_ips_shape_sf  = \"get_1D_binContent(h_ds_ips_shape_sf,C_{}_BS_ips_xy)\".format(config[\"mu1_label\"])\n",
    "    df = df.Define(\"C_ds_ips_shape_sf\",str(ds_ips_shape_sf)) \n",
    "\n",
    "# define mu_Hnl IPS shape scale factors for MC only\n",
    "if dataset_category != \"data\" and applyMuHnlIPSCorr:\n",
    "    hnl_ips_shape_sf  = \"get_1D_binContent(h_hnl_ips_shape_sf,C_{}_BS_ips_xy)\".format(config[\"mu2_label\"])\n",
    "    df = df.Define(\"C_hnl_ips_shape_sf\",str(hnl_ips_shape_sf)) \n",
    "\n",
    "if bestCandChecks:\n",
    "    hmodel = (\"h_mult_input\",\";Candidate multiplicity;Normalized to unit\",20,1.,21.)\n",
    "    df_check = df.Define(\"mult_input\",\"get_cand_multiplicity(C_Hnl_mass)\")\n",
    "    df_check.Histo1D(hmodel,\"mult_input\").SaveAs(\"h_cand_mult_input_{}.root\".format(dataset_to_process))\n",
    "    df_check = df_check.Filter(\"mult_input>1\",\"fraction of input events with more than 1 candidate\")\n",
    "    # define a index selecting best candidate in the event\n",
    "    df_check = df_check.Define(selection[\"best_cand_var\"][\"name\"],selection[\"best_cand_var\"][\"definition\"])\n",
    "    \n",
    "    # redefine variables so that only best candidate is saved\n",
    "    #for c in df_check.GetColumnNames():\n",
    "    #    col_name = str(c)\n",
    "    #    col_type = df_check.GetColumnType(col_name)\n",
    "        # choose candidate branches (beginning with 'C_')\n",
    "    #    if (not col_name.find(\"C_\")<0) and (not col_type.find(\"ROOT::VecOps\")<0):\n",
    "    #        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "    #        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "    #        continue\n",
    "    for col_name in good_cols:\n",
    "        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "\n",
    "    if dataset_category == \"signal\":\n",
    "        df_check = df_check.Filter(\"C_pass_gen_matching\",\"best-cand-input events have a GEN-matched candidate\")\n",
    "#    df_check.Report().Print()\n",
    "\n",
    "######################\n",
    "#### PRESELECTION ####\n",
    "######################\n",
    "\n",
    "#apply common pre-selection\n",
    "presel_i = 0\n",
    "presel_cuts = list()\n",
    "for sel in selection[\"preselection_cuts\"]:\n",
    "    mask_var = \"pass_presel_\"+str(presel_i)\n",
    "    presel_cuts.append(mask_var)\n",
    "    df = df.Define(mask_var,sel[\"cut\"])\n",
    "    df = df.Filter(\"ROOT::VecOps::Any(\"+mask_var+\")\",sel[\"printout\"])\n",
    "    presel_i+=1\n",
    "\n",
    "# skim vectors to retain only candidates passing preselection\n",
    "#for c in df.GetColumnNames():\n",
    "#    col_name = str(c)\n",
    "#    col_type = df.GetColumnType(col_name)\n",
    "    # choose candidate branches (beginning with 'C_')\n",
    "#    if(hnl_tools.is_good_cand_var(col_name) and (not col_type.find(\"ROOT::VecOps\")<0)): \n",
    "#        presel_cuts_AND = \"&&\".join(presel_cuts)  \n",
    "#        df = df.Redefine(col_name,col_name+\"[\"+presel_cuts_AND+\"]\")\n",
    "#        continue\n",
    "for col_name in good_cols:\n",
    "    # choose candidate branches (beginning with 'C_')\n",
    "    if hnl_tools.is_good_cand_var(col_name): \n",
    "        presel_cuts_AND = \"&&\".join(presel_cuts)  \n",
    "        df = df.Redefine(col_name,col_name+\"[\"+presel_cuts_AND+\"]\")\n",
    "\n",
    "# count how many preselected events have at least a GEN-matched candidate\n",
    "if bestCandChecks:\n",
    "    hmodel = (\"h_mult_presel\",\";Candidate multiplicity;Normalized to unit\",20,1.,21.)\n",
    "    df_check = df.Define(\"mult_presel\",\"get_cand_multiplicity(C_Hnl_mass)\")\n",
    "    df_check.Histo1D(hmodel,\"mult_presel\").SaveAs(\"h_cand_mult_presel_{}.root\".format(dataset_to_process))\n",
    "    df_check = df_check.Filter(\"mult_presel>1\",\"fraction of preselected events with more than 1 candidate\")\n",
    "    # define a index selecting best candidate in the event\n",
    "    df_check = df_check.Define(selection[\"best_cand_var\"][\"name\"],selection[\"best_cand_var\"][\"definition\"])\n",
    "    \n",
    "    # redefine variables so that only best candidate is saved\n",
    "    #for c in df_check.GetColumnNames():\n",
    "    #    col_name = str(c)\n",
    "    #    col_type = df_check.GetColumnType(col_name)\n",
    "        # choose candidate branches (beginning with 'C_')\n",
    "    #    if (not col_name.find(\"C_\")<0) and (not col_type.find(\"ROOT::VecOps\")<0):\n",
    "    #        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "    #        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "    #        continue\n",
    "    for col_name in good_cols:   \n",
    "        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "\n",
    "    if dataset_category == \"signal\":\n",
    "        df_check = df_check.Filter(\"C_pass_gen_matching\",\"best-cand-preselected events have at least a GEN-matched candidate\")\n",
    "    #df_check.Report().Print()\n",
    "\n",
    "#save preselected tree\n",
    "if savePreselectedTree:\n",
    "    finalTree_outputFileName = \"tree_\"+dataset_name_label+\".root\"\n",
    "    if not saveRemotely:\n",
    "        finalTree_outputDirName = os.path.join(config[\"preselected_tree_output_dir_name\"],dataset_name_label)\n",
    "    else:\n",
    "        finalTree_outputDirName = os.path.join(config[\"remote_preselected_tree_output_dir_name\"],dataset_name_label)\n",
    "    if tag!=\"\":\n",
    "        if not saveRemotely:\n",
    "            finalTree_outputFileName = \"tree_\"+tag+\"_\"+dataset_name_label+\".root\"\n",
    "        else:\n",
    "            finalTree_outputFileName = \"tree_\"+tag+\"_\"+dataset_name_label[:-5]+\".root\"\n",
    "    if not saveRemotely and not distributed:\n",
    "        subprocess.call(['mkdir','-p',finalTree_outputDirName])\n",
    "    finalTree_outFullPath = os.path.join(finalTree_outputDirName,finalTree_outputFileName)\n",
    "    var_list = ListOfColumns #df.GetColumnNames()\n",
    "    var_keep_list = keep\n",
    "    if len(var_keep_list)>0:\n",
    "        var_list = var_keep_list\n",
    "    if not saveRemotely:\n",
    "        if not distributed:\n",
    "            df.Snapshot(config[\"tree_output_name\"],finalTree_outFullPath,var_list)\n",
    "            nevt = nEvents.GetValue()\n",
    "        else:\n",
    "            print(\"Impossible to save the preselected tree locally when running distributed. Turn on saveRemotely option and re-run.\")\n",
    "    else:\n",
    "        finalTree_outFullPath2 = \"https://\" + config[\"redirector\"] + finalTree_outputDirName\n",
    "        finalTree_outFullPath = \"root://t2-xrdcms.lnl.infn.it:7070/\" + finalTree_outputDirName\n",
    "        df.Snapshot(config[\"tree_output_name\"],finalTree_outputFileName, var_list)\n",
    "        nevt = nEvents.GetValue()\n",
    "    post_start_step2 = time.time() \n",
    "    if saveRemotely:\n",
    "        if distributed:\n",
    "            client.run(transfer_to_tier, remote_folder_name=finalTree_outFullPath2)\n",
    "        else:\n",
    "            transfer_to_tier(remote_folder_name=finalTree_outFullPath2)\n",
    "    if not(not saveRemotely and distributed):\n",
    "        print(\"Preselected tree saved in {}\".format(finalTree_outFullPath))\n",
    "        \n",
    "post_stop_step2 = time.time()\n",
    "###################\n",
    "#### SELECTION ####\n",
    "###################\n",
    "\n",
    "#apply optimized selection for each category\n",
    "sel_cuts = dict()\n",
    "for cat in selection[\"categories\"]:\n",
    "    l = list()\n",
    "    sel_i = 0\n",
    "    for cut in cat[\"selection_cuts\"]:\n",
    "        mask_var = \"pass_sel_\"+cat[\"label\"]+\"_\"+str(sel_i)\n",
    "        l.append(mask_var)\n",
    "        df = df.Define(mask_var,cut[\"cut\"]+' && '+cat[\"cut\"])\n",
    "        sel_i+=1\n",
    "    sel_cuts[cat[\"label\"]] = l\n",
    "\n",
    "# build AND of each category's selection cuts\n",
    "sel_cuts_AND = list()\n",
    "for cat in sel_cuts:\n",
    "    s = \"&&\".join(sel_cuts[cat])\n",
    "    sel_cuts_AND.append(s)\n",
    "\n",
    "# filter events which do not pass any of the categories' selection cuts\n",
    "df = df.Filter(\"ROOT::VecOps::Any(\"+\"||\".join(sel_cuts_AND)+\")\",\"selection cuts\") \n",
    "\n",
    "# skim vectors to retain only candidates passing selection cuts\n",
    "#for c in df.GetColumnNames():\n",
    "#    col_name = str(c)\n",
    "#    col_type = df.GetColumnType(col_name)\n",
    "    # choose candidate branches (beginning with 'C_')\n",
    "#    if hnl_tools.is_good_cand_var(col_name) and col_type.find(\"ROOT::VecOps\")==0: \n",
    "#        df = df.Redefine(col_name,col_name+\"[\"+\"||\".join(sel_cuts_AND)+\"]\")\n",
    "for col_name in good_cols:\n",
    "    if hnl_tools.is_good_cand_var(col_name): \n",
    "        df = df.Redefine(col_name,col_name+\"[\"+\"||\".join(sel_cuts_AND)+\"]\")\n",
    "\n",
    "# count how many selected events have at least a GEN-matched candidate\n",
    "if bestCandChecks:\n",
    "    hmodel = (\"h_mult_sel\",\";Candidate multiplicity;Normalized to unit\",20,1.,21.)\n",
    "    df_check = df.Define(\"mult_sel\",\"get_cand_multiplicity(C_Hnl_mass)\")\n",
    "    df_check.Histo1D(hmodel,\"mult_sel\").SaveAs(\"h_cand_mult_sel_{}.root\".format(dataset_to_process))\n",
    "    df_check = df_check.Filter(\"mult_sel>1\",\"fraction of selected events with more than 1 candidate\")\n",
    "    #vl = df.GetColumnNames()\n",
    "    #df_check.Snapshot(\"test\",\"test.root\",vl)\n",
    "    # define a index selecting best candidate in the event\n",
    "    df_check = df_check.Define(selection[\"best_cand_var\"][\"name\"],selection[\"best_cand_var\"][\"definition\"])\n",
    "    \n",
    "    # redefine variables so that only best candidate is saved\n",
    "    #for c in df_check.GetColumnNames():\n",
    "    #    col_name = str(c)\n",
    "    #    col_type = df_check.GetColumnType(col_name)\n",
    "        # choose candidate branches (beginning with 'C_')\n",
    "    #    if (not col_name.find(\"C_\")<0) and (not col_type.find(\"ROOT::VecOps\")<0):\n",
    "    #        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "    #        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "    #        continue\n",
    "    for col_name in good_cols:\n",
    "        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "        df_check = df_check.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "    if dataset_category == \"signal\":\n",
    "        df_check = df_check.Filter(\"C_pass_gen_matching\",\"best-cand-selected events have at least a GEN-matched candidate\")\n",
    "    #df_check.Report().Print()\n",
    "\n",
    "###################\n",
    "#### BEST CAND ####\n",
    "###################\n",
    "\n",
    "# define a index selecting best candidate in the event\n",
    "df = df.Define(selection[\"best_cand_var\"][\"name\"],selection[\"best_cand_var\"][\"definition\"])\n",
    "\n",
    "# redefine variables so that only best candidate is saved\n",
    "#for c in df.GetColumnNames():\n",
    "#    col_name = str(c)\n",
    "#    col_type = df.GetColumnType(col_name)\n",
    "    # choose candidate branches (beginning with 'C_')\n",
    "#    if col_name.find(\"C_\")==0 and col_type.find(\"ROOT::VecOps\")==0:\n",
    "#        idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "#        df = df.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "#        continue\n",
    "for col_name in good_cols:\n",
    "    idx = str(selection[\"best_cand_var\"][\"name\"])\n",
    "    df = df.Redefine(col_name,col_name+\"[\"+idx+\"]\")\n",
    "\n",
    "if dataset_category == \"signal\":\n",
    "    df = df.Filter(\"C_pass_gen_matching\",\"best-candidate-selected events have at least a GEN-matched candidate\")\n",
    "\n",
    "#define categories\n",
    "df = df.Define(\"C_cat\",\"get_lxy_categories(C_Hnl_vertex_2DDist_BS,C_mu_Hnl_charge,C_mu_Ds_charge)\")\n",
    "\n",
    "# redefine here the total weight as the product of all weights previously defined\n",
    "\n",
    "# Include trigger SF\n",
    "if dataset_category != \"data\" and not skipTrigSF:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_trigger_sf\")\n",
    "\n",
    "# Include ID scale factors\n",
    "if dataset_category != \"data\" and not skipMuIDsf:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_{mu1l}_id_sf*C_{mu2l}_id_sf\".format(mu1l=config[\"mu1_label\"],mu2l=config[\"mu2_label\"]))\n",
    "\n",
    "# Include RECO scale factors\n",
    "if dataset_category != \"data\" and not skipMuRecosf:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_{mu1l}_reco_sf*C_{mu2l}_reco_sf\".format(mu1l=config[\"mu1_label\"],mu2l=config[\"mu2_label\"]))\n",
    "\n",
    "# Include mu_Ds pt shape scale factors\n",
    "if dataset_category != \"data\" and applyMuDsPtCorr:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_ds_pt_shape_sf\")\n",
    "\n",
    "# Include mu_Hnl pt shape scale factors\n",
    "if dataset_category != \"data\" and applyMuHnlPtCorr:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_hnl_pt_shape_sf\")\n",
    "\n",
    "# Include mu_Ds IPS shape scale factors\n",
    "if dataset_category != \"data\" and applyMuDsIPSCorr:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_ds_ips_shape_sf\")\n",
    " \n",
    "# Include mu_Hnl IPS shape scale factors\n",
    "if dataset_category != \"data\" and applyMuHnlIPSCorr:\n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*C_hnl_ips_shape_sf\")\n",
    "\n",
    "#################\n",
    "##### SAVE ######\n",
    "#################\n",
    "\n",
    "if saveOutputTree:\n",
    "    finalTree_outputFileName = \"tree_\"+dataset_name_label+\".root\"\n",
    "    finalCSV_outputFileName  = \"tree_\"+dataset_name_label+\".csv\"\n",
    "    if not saveRemotely:\n",
    "        finalTree_outputDirName = os.path.join(config[\"tree_output_dir_name\"],dataset_name_label)\n",
    "    else:\n",
    "        finalTree_outputDirName = os.path.join(config[\"remote_tree_output_dir_name\"],dataset_name_label)\n",
    "    if tag!=\"\":\n",
    "        if not saveRemotely:\n",
    "            finalTree_outputFileName = \"tree_\"+tag+\"_\"+dataset_name_label+\".root\"\n",
    "            finalCSV_outputFileName  = \"tree_\"+tag+\"_\"+dataset_name_label+\".csv\"\n",
    "        else:\n",
    "            finalTree_outputFileName = \"tree_\"+tag+\"_\"+dataset_name_label[:-5]+\".root\"\n",
    "            finalCSV_outputFileName  = \"tree_\"+tag+\"_\"+dataset_name_label[:-5]+\".csv\"\n",
    "    if not saveRemotely and not distributed:\n",
    "        subprocess.call(['mkdir','-p',finalTree_outputDirName])\n",
    "    finalTree_outFullPath = os.path.join(finalTree_outputDirName,finalTree_outputFileName)\n",
    "    finalCSV_outFullPath = os.path.join(finalTree_outputDirName,finalCSV_outputFileName)\n",
    "\n",
    "    #save output tree\n",
    "    var_list = ListOfColumns #df.GetColumnNames()\n",
    "    csv_var_list = [x for x in ListOfColumns if hnl_tools.is_good_cand_var(x) or x.find(\"tot_weight\")==0 or x.find(\"mc_weight\")==0] #[x for x in df.GetColumnNames() if hnl_tools.is_good_cand_var(x) or x.find(\"tot_weight\")==0 or x.find(\"mc_weight\")==0]\n",
    "    var_keep_list = keep\n",
    "    if len(var_keep_list)>0:\n",
    "        var_list = var_keep_list\n",
    "        csv_var_list = var_keep_list  \n",
    "    if not saveRemotely:\n",
    "        if not distributed:\n",
    "            df.Snapshot(config[\"tree_output_name\"],finalTree_outFullPath,var_list)\n",
    "            nevt = nEvents.GetValue()\n",
    "        else:\n",
    "            print(\"Impossible to save the output tree locally when running distributed. Turn on saveRemotely option and re-run.\")      \n",
    "    else:\n",
    "        finalTree_outFullPath2 = \"https://\" + config[\"redirector\"] + finalTree_outputDirName\n",
    "        finalTree_outFullPath = \"root://t2-xrdcms.lnl.infn.it:7070/\" + finalTree_outputDirName\n",
    "        df.Snapshot(config[\"tree_output_name\"],finalTree_outputFileName,var_list)\n",
    "        nevt = nEvents.GetValue()\n",
    "    post_start_step3 = time.time()\n",
    "    \n",
    "    if saveRemotely:\n",
    "        if distributed:\n",
    "            client.run(transfer_to_tier, remote_folder_name=finalTree_outFullPath2)\n",
    "        else:\n",
    "            transfer_to_tier(remote_folder_name=finalTree_outFullPath2)\n",
    "    \n",
    "    #save output csv\n",
    "    if not(not saveRemotely and distributed):\n",
    "        a = df.AsNumpy(csv_var_list)\n",
    "        pdf = pd.DataFrame(a)\n",
    "        \n",
    "    if not saveRemotely:\n",
    "        if not distributed:\n",
    "            pdf.to_csv(finalCSV_outFullPath, index=False)\n",
    "    else:\n",
    "        finalCSV_outFullPath2 = \"https://\" + config[\"redirector\"] + finalCSV_outFullPath\n",
    "        finalCSV_outFullPath = \"root://t2-xrdcms.lnl.infn.it:7070/\" + finalCSV_outFullPath\n",
    "        pdf.to_csv(finalCSV_outputFileName, index=False)\n",
    "        os.system(\"davix-put \" + finalCSV_outputFileName + \" \" + finalCSV_outFullPath2 + \" -E $X509_USER_PROXY --capath $X509_CERT_DIR\")\n",
    "        \n",
    "    if not(not saveRemotely and distributed):\n",
    "        print(\"Output tree saved in {}\".format(finalTree_outFullPath))\n",
    "        print(\"Output csv saved in {}\".format(finalCSV_outFullPath))\n",
    "\n",
    "        #update json\n",
    "        key = \"final_file_name_list\"\n",
    "        if tag != \"\":\n",
    "            key = key+\"_\"+str(tag)\n",
    "        if saveRemotely:\n",
    "            ls_output = os.popen(\"davix-ls \" + finalTree_outFullPath2 + \" -E $X509_USER_PROXY --capath $X509_CERT_DIR\")\n",
    "            fileNames = ls_output.read().splitlines()\n",
    "            ntuples[dataset_to_process][key] = [str(finalTree_outFullPath) + \"/\" + fileName for fileName in fileNames if fileName.endswith(\".root\")]\n",
    "        else:\n",
    "            ntuples[dataset_to_process][key] = [str(finalTree_outFullPath)]\n",
    "        with open(config[\"ntuples_cfg_file_full_path\"], \"w\") as f:\n",
    "            json.dump(ntuples,f, indent=4, sort_keys=True)\n",
    "        print(\"{} updated\".format(config[\"ntuples_cfg_file_full_path\"]))\n",
    "    post_stop_step3 = time.time()\n",
    "##################\n",
    "##### SPLOT ######\n",
    "##################\n",
    "\n",
    "if dataset_category==\"data\" and addSPlotWeight:\n",
    "    print(\"Input splot weight file: {}\".format(ntuples[dataset_to_process][\"splot_weight_input_file\"]))\n",
    "    sdf = ROOT.RDataFrame(ntuples[dataset_to_process][\"splot_weight_tree_name\"],ntuples[dataset_to_process][\"splot_weight_input_file\"]) # get tree containing splot weights\n",
    "    asw = sdf.AsNumpy([ntuples[dataset_to_process][\"splot_weight_variable\"]])[ntuples[dataset_to_process][\"splot_weight_variable\"]] # get column of splot weights\n",
    "    print(\"entries in splot tree: {}\".format(len(asw)))\n",
    "    print(\"entries in analyzed tree: {}\".format(df.Count().GetValue()))\n",
    "    if len(asw) != df.Count().GetValue():\n",
    "        print(\"!!! splot tree and analyzed tree do not have the same number of entries !!!\")\n",
    "        print(\"!!! please check that you are using the correct input tree !!!\")\n",
    "        exit()\n",
    "    df = df.Define(\"splot_weight\",'auto to_eval = std::string(\"asw[\") + std::to_string(rdfentry_) + \"]\"; return float(TPython::Eval(to_eval.c_str()));') \n",
    "    df = df.Redefine(\"tot_weight\",\"tot_weight*splot_weight\")\n",
    "\n",
    "#######################\n",
    "##### HISTOGRAMS ######\n",
    "#######################\n",
    "histo_start = time.time()\n",
    "if doHistograms:\n",
    "    if not(not saveRemotely and distributed):\n",
    "        hnl_tools.do_histos(df, config, dataset_name_label, saveRemotely=saveRemotely, tag=\"\")\n",
    "    else:\n",
    "        print(\"Impossible to save the histograms locally when running distributed. Turn on saveRemotely option and re-run.\")\n",
    "        \n",
    "histo_stop = time.time()\n",
    "# TODO:define per category reports\n",
    "# print(\"+++ FINAL REPORT +++\")\n",
    "# report= df.Report()\n",
    "# report.Print()\n",
    "\n",
    "###################################\n",
    "####### SAVE MONITORING DATA ######\n",
    "# (Use correct Dask sched image!) #\n",
    "###################################\n",
    "\n",
    "if distributed and saveRemotely:\n",
    "    client.run(transfer_to_tier_monitoring, remote_folder_name=finalTree_outFullPath2)\n",
    "    \n",
    "############################################\n",
    "# CLEAR LOCAL COPIES (when saved remotely) #\n",
    "############################################\n",
    "\n",
    "if saveRemotely:\n",
    "    os.system(\"rm ./tree_*.csv\")\n",
    "    if doHistograms:\n",
    "        os.system(\"rm ./histograms_*.root\")\n",
    "    if not distributed:\n",
    "        os.system(\"rm ./tree_*.root\")\n",
    "\n",
    "#############\n",
    "#### END ####\n",
    "#############\n",
    "stop_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb4cb9-4828-4268-9837-ccb1dde8f7fa",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdcd54-6d5d-4c1a-9f5e-a7581ecefe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Analysis completed in {} seconds ---\".format(stop_time - pre_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac34bf-c48e-48b7-8e92-d91d4c8a866a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Writing time information on JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf332c-e2ac-4f25-b9c4-7fd019276aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRESELECTION\n",
    "pre_time = pre_stop - pre_start\n",
    "loop_time = post_start_step2 - pre_stop\n",
    "post_time = post_stop_step2 - post_start_step2\n",
    "\n",
    "print(pre_time)\n",
    "print(loop_time)\n",
    "print(post_time)\n",
    "\n",
    "print(\"Total time of execution: \" + str(stop_time-pre_start))\n",
    "print(\"Sum of components: \" + str(pre_time + loop_time + post_time))\n",
    "\n",
    "with open('metrics.json','w') as out_file:\n",
    "    json.dump({'pre_time': pre_time, \n",
    "                'analysis_time': loop_time, \n",
    "                'post_time': post_time,\n",
    "                'n_events': nevt\n",
    "               }, out_file, sort_keys=True, indent=4)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028b75e-f2cb-46e6-9141-0d1f385d7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTION\n",
    "pre_time = pre_stop - pre_start\n",
    "loop_time = (post_start_step3 - post_stop_step2) + (histo_stop - post_stop_step3)\n",
    "post_time = (post_stop_step3 - post_start_step3) + (stop_time-histo_stop)\n",
    "\n",
    "print(pre_time)\n",
    "print(loop_time)\n",
    "print(post_time)\n",
    "\n",
    "print(\"Total time of execution: \" + str(stop_time-pre_start))\n",
    "print(\"Sum of components: \" + str(pre_time + loop_time + post_time))\n",
    "\n",
    "with open('metrics.json','w') as out_file:\n",
    "    json.dump({'pre_time': pre_time, \n",
    "                'loop_time': loop_time, \n",
    "                'post_time': post_time,\n",
    "                'n_events': nevt\n",
    "                }, out_file, sort_keys=True, indent=4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07c0c6-a446-460d-a7c7-ec450605ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRESELECTION + SELECTION\n",
    "pre_time = pre_stop - pre_start\n",
    "loop_time = (post_start_step2 - pre_stop) + (post_start_step3 - post_stop_step2) + (histo_stop - post_stop_step3)\n",
    "post_time = (post_stop_step2 - post_start_step2) + (post_stop_step3 - post_start_step3) + (stop_time-histo_stop)\n",
    "\n",
    "print(pre_time)\n",
    "print(loop_time)\n",
    "print(post_time)\n",
    "\n",
    "print(\"Total time of execution: \" + str(stop_time-pre_start))\n",
    "print(\"Sum of components: \" + str(pre_time + loop_time + post_time))\n",
    "\n",
    "with open('metrics.json','w') as out_file:\n",
    "    json.dump({'pre_time': pre_time, \n",
    "                'loop_time': loop_time, \n",
    "                'post_time': post_time,\n",
    "                'n_events': nevt\n",
    "                }, out_file, sort_keys=True, indent=4)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c11f2-cf23-4dde-a4ef-40453a22e534",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Additional Commands and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a5be7-c936-4fe8-993c-c8cdd8d862df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91baf42f-9ec4-4d69-8d09-708de986be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_outFullPath2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4e51a-2eb7-4b25-81cf-7a00fe3e996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443573a-581f-46b7-82f8-dbc69857b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rootls -lt root://cms-xrd-global.cern.ch//store/user/llunerti/DsToNMu_NToMuPi_SoftQCD_mN1p5_ctau100p0mm_TuneCP5_13TeV-pythia8-evtgen/DsToHnlMu_HnlToMuPi_prompt_UL/221107_101012/0000/DsToHnlMu_HnlToMuPi_prompt_DsToNMu_NToMuPi_SoftQCD_mN1p5_ctau100p0mm_TuneCP5_13TeV-pythia8-evtgen_tree_1.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cb715-1136-4712-9ad8-46d43696b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!davix-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09458e8-245a-4448-a72c-20057efa3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "numWorkers= len(client.scheduler_info()['workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee15a7-1084-4d91-91aa-53b3354bd2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize chain\n",
    "#chain = ROOT.TChain(tree_name)\n",
    "#tot_file = len(inputFileName_list)\n",
    "#files_in_the_chain = int(0)\n",
    "\n",
    "#add file to the chain\n",
    "#for inputFileName in inputFileName_list:\n",
    "#    print(\"Adding {} to the chain...\".format(inputFileName))\n",
    "#    chain.Add(inputFileName)\n",
    "#    files_in_the_chain += 1\n",
    "#    print(\"[{}/{}] files added to the chain\".format(files_in_the_chain,tot_file))\n",
    "#    print(\"{} entries now in the chain\".format(chain.GetEntries()))\n",
    "\n",
    "#print(\"\\n\")\n",
    "#print(\"{} total entries ...\".format(chain.GetEntries()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
